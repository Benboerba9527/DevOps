# K8s测试环境上线应用的完整准备与配置方案大纲

## 一、K8s集群完善与基础设施配置

1. **网络与存储**
   - 配置集群网络插件（如 Calico/Flannel）。
   k8s-v1.28.2集群使用的是Flannel作为网络插件


   - 部署 NFS-CSI 插件，实现持久化存储（为数据库、应用等提供 PVC 支持）。
   - 配置本地 NFS 服务或其他存储后端。
——————————————————————————————————————————————————————————————————————————————————————————————————————
部署方案
k8s集群使用NFS-CSI插件外加nfs-server来实现持久存储
一、集群外节点部署NFS-Server服务更有利于管理以及数据安全，当k8s集群节重启时nfs-server服务也不会受影响，这里是测试环境，仅使用harbor的单节点部署nfs-server服务
Ubuntu 22.04 部署 NFS 服务端步骤
sudo apt update
sudo apt install -y nfs-kernel-server
sudo mkdir -p /data/nfs/k8s
sudo chown -R nobody:nogroup /data/nfs/k8s
echo "/data/nfs/k8s *(rw,sync,no_root_squash,no_subtree_check)" | sudo tee -a /etc/exports
sudo exportfs -rav
sudo systemctl enable --now nfs-server

PS: NFS服务默认端口是2049

二、测试NFS服务是否可用
在任意k8s节点上测试即可，这里使用的是k8s-master-1节点
1、安装nfs-common工具
apt install -y nfs-common

2、查看NFS共享目录
showmount -e 192.168.26.99      #如果NFS服务正常则可以看到上面设置的共享目录/data/nfs/k8s

3、手动挂载测试
sudo mkdir -p /mnt/nfs-test
sudo mount -t nfs 192.168.26.99:/data/nfs/k8s /mnt/nfs-test

4、读写测试
echo 123 > /mnt/nfs-test/testfile
cat /mnt/nfs-test/testfile

如果读写测试正常则NFS服务端可用

5、卸载测试挂载点
umount /mnt/nfs-test


三、为定制化食谱应用创建独立的共享目录

1、服务端创建共享目录
mkdir -pv /data/nfs/food_menu

echo "/data/nfs/food_menu *(rw,sync,no_root_squash,no_subtree_check)" | sudo tee -a /etc/exports

exportfs -rav  # 生效新的共享目录配置


2、k8s部署NFS-CSI并为应用创建共享目录
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/

helm repo update

helm repo list

# 在nfs-csi名称空间下部署,helm部署默认是以deployment形式部署在集群中且是单副本，若需高可用需手动修改deployment的replicas,切记需提前在k8s节点尤其是worker节点安装nfs-common工具，否则会显示挂载失败

 Warning  FailedMount  98s (x9 over 3m46s)  kubelet            MountVolume.SetUp failed for volume "nfs-subdir-external-provisioner-root" : mount failed: exit status 32

helm install nfs-csi nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
--set nfs.server=192.168.26.99 \
--set nfs.path=/data/nfs/food_menu \
-n nfs-csi --create-namespace


3、为应用创建storageClass提供持久化存储
# 按照以下步骤为应用创建StorageClass并实现基于NFS-CSI的持久化存储

a. 创建StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: custom-food-menu-mysql
provisioner: cluster.local/nfs-csi-nfs-subdir-external-provisioner
parameters:
  server: 192.168.26.99
  share: /data/nfs/food_menu
reclaimPolicy: Retain
mountOptions:
  - vers=4.1

kubectl apply -f 01_food_menu_storageclass.yaml

b. 创建 PVC（持久卷声明）
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: food-menu-pvc
  namespace: custom-food-menu
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: custom-food-menu-mysql
  resources:
    requests:
      storage: 20Gi

kubectl apply -f nfs-pvc.yaml

c. 在应用 Pod/Deployment 中挂载 PVC,以下仅为示例
在你的 Deployment 或 Pod 的 spec 里添加如下 volume 和 volumeMounts：
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  namespace: custom-food-menu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: 192.168.26.99/custom_food_menu/mysql:5.7
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: rootpassword
        - name: MYSQL_DATABASE
          value: food_menu_db
        ports:
        - containerPort: 3306
        volumeMounts:
        - mountPath: /var/lib/mysql
          name: mysql-pv-storage
      volumes:
      - name: mysql-pv-storage
        persistentVolumeClaim:
          claimName: food-menu-pvc

kubectl apply -f dynimic-pv-mysql.yaml

pod正常创建并自动挂载动态pv，说明使用基于NFS-Server的持久化存储方案正常



——————————————————————————————————————————————————————————————————————————————————————————————————————

2. **Ingress与负载均衡**
   - 部署 Ingress Controller（如 Nginx Ingress、Traefik）。
   - 部署 OpenELB 或 MetalLB（如需 LoadBalancer 类型服务）。
   - 配置 Ingress 资源，实现域名访问。

# ingress实现七层代理，适用于需要基于域名访问的服务场景，需通过部署ingress controller组件来定义规则
# OpenELB是LoadBalancer服务的一种，可将集群内的一段地址设置为EIP提供给k8s服务使用作为集群外访问的地址

# OpenELB部署
      1、wget https://raw.githubusercontent.com/openelb/openelb/release-0.6/deploy/openelb.yaml

      2、kubectl apply -f openelb.yaml

      3、kubectl get po -n openelb-system
      kubectl get pod -n openelb-system -owide 
      NAME                                 READY   STATUS      RESTARTS   AGE    IP              NODE           NOMINATED NODE   READINESS GATES
      openelb-admission-create-56tb5       0/1     Completed   0          111s   10.244.5.21     k8s-worker-3   <none>           <none>
      openelb-admission-patch-kbwcw        0/1     Completed   0          111s   10.244.4.24     k8s-worker-2   <none>           <none>
      openelb-controller-d9f4b687b-zwxv6   1/1     Running     0          111s   10.244.5.22     k8s-worker-3   <none>           <none>
      openelb-speaker-5d2j2                1/1     Running     0          111s   192.168.26.21   k8s-master-1   <none>           <none>
      openelb-speaker-kcbjw                1/1     Running     0          111s   192.168.26.26   k8s-worker-3   <none>           <none>
      openelb-speaker-nxp2l                1/1     Running     0          111s   192.168.26.25   k8s-worker-2   <none>           <none>
      openelb-speaker-pct5l                1/1     Running     0          111s   192.168.26.23   k8s-master-3   <none>           <none>
      openelb-speaker-q9t2x                1/1     Running     0          111s   192.168.26.24   k8s-worker-1   <none>           <none>
      openelb-speaker-s47vh                1/1     Running     0          111s   192.168.26.22   k8s-master-2   <none>           <none>

      4、创建EIP地址池并测试OpenELB功能是否正常
      

3. **镜像仓库**
   - 已部署本地 Harbor，配置 K8s 节点可信 Harbor 证书与镜像拉取 secret。

4. **配置中心**
   - 部署 Nacos（推荐使用 Helm Chart 或 Operator）。
   - 配置 Nacos 持久化存储与高可用（如有需要）。
   - 配置应用与 Nacos 的集成（环境变量、配置文件等）。

5. **监控与日志**
   - 部署 Prometheus + Grafana（监控 K8s 资源、应用、数据库等）。
   - 部署 kube-state-metrics、node-exporter。
   - 部署 Alertmanager（告警）。
   - 部署 Loki/ELK/EFK（日志采集与查询）。

6. **自动化发布与运维平台**
   - 部署 KubeSphere、Rancher 或 ArgoCD、Jenkins（任选其一或组合）。
   - 配置平台对接 Harbor、K8s 集群、存储、Ingress。
   - 配置 RBAC 权限与用户管理。

---

## 二、自动化发布部署流程

1. **CI/CD流水线配置**
   - 代码提交触发 Jenkins/ArgoCD/KubeSphere 流水线。
   - 自动构建 Docker 镜像并推送至 Harbor。
   - 自动生成/更新 K8s 部署 YAML 或 Helm Chart。
   - 自动发布到 K8s 集群，回滚与灰度支持。

2. **页面化/自助化部署**
   - 通过 KubeSphere/Rancher Web UI 选择镜像、配置参数、发布应用。
   - 支持 Helm Chart/YAML 文件上传与参数化部署。
   - 支持多环境（dev/test/prod）一键切换与部署。

3. **应用配置与服务治理**
   - 应用通过 Nacos 动态获取配置。
   - 支持配置热更新、服务注册与发现。

---

## 三、监控与运维

1. **集群与应用监控**
   - Prometheus 采集集群、节点、Pod、Service、Ingress、数据库等指标。
   - Grafana 可视化展示，配置仪表盘。
   - Alertmanager 配置邮件、钉钉等告警通道。

2. **日志采集与分析**
   - 部署 Loki/ELK/EFK，采集 K8s 容器日志。
   - 配置日志查询、告警与归档。

3. **健康检查与自动修复**
   - 配置 liveness/readiness 探针。
   - 配置 HPA/VPA 自动伸缩。

---

## 四、环境安全与合规

1. **权限与安全**
   - 配置 RBAC，最小权限原则。
   - 配置 Harbor 镜像仓库访问控制。
   - 配置 Ingress HTTPS 证书。

2. **备份与恢复**
   - 配置数据库、Nacos、重要数据的定期备份。
   - 配置 K8s 资源的备份方案（如 Velero）。

---

## 五、上线定制化食谱应用的具体步骤

1. **准备应用镜像并推送 Harbor**
2. **准备数据库（MySQL）与数据导入**
3. **编写/准备 K8s 部署 YAML 或 Helm Chart**
4. **通过自动化平台（如 KubeSphere/Rancher/ArgoCD）发布应用**
5. **配置 Ingress，实现域名访问**
6. **应用对接 Nacos 配置中心**
7. **接入监控与日志平台**
8. **验证功能、监控、日志、告警等是否正常**

---

> 以上为本地K8s测试环境上线应用的完整准备与配置大纲，涵盖基础设施、自动化、配置中心、监控、日志、安全等全流程。